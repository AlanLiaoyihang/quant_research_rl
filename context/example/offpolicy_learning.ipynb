{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Learning in Contextual Bandits \n",
    "\n",
    "** *\n",
    "\n",
    "This IPython notebook illustrates the usage of the [contextualbandits](https://www.github.com/david-cortes/contextualbandits) package's `offpolicy` module through a simulation with a public dataset.\n",
    "\n",
    "\n",
    "**Small note: if the TOC here is not clickable or the math symbols don't show properly, try visualizing this same notebook from nbviewer following [this link](http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/offpolicy_learning.ipynb).**\n",
    "\n",
    "** *\n",
    "\n",
    "### Sections \n",
    "\n",
    "[1. Problem description](#p1)\n",
    "\n",
    "[2. Algorithms](#p2)\n",
    "\n",
    "[3. Experiments](#p3)\n",
    "\n",
    "[4. References](#p4)\n",
    "\n",
    "** * \n",
    "\n",
    "<a id=\"p1\"></a>\n",
    "## 1. Problem description\n",
    "\n",
    "For a general description of the contextual bandits problem, see the first part of the package's guide [Online Contextual Bandits](http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/online_contextual_bandits.ipynb).\n",
    "\n",
    "Unlike the `online` module mentioned above, this module deals with a slightly different problem: once we have collected (biased) data following some policy – consisting in features observed, actions chosen, rewards observed, and ideally scores or estimated rewards probabilities that the exploration policy predicted – how can we now build a different and perhaps better policy? (also known as \"Off-policy learning\").\n",
    "\n",
    "This module focuses on building non-online, exploit-only policies, and unfortunately, the algorithms don't extend nicely to the case of classifiers that allow some exploration. It assumes a stationary exploration policy (non-online) for the methods to work well in theory, but in practice they can also work with data collected through online policies whose reward estimates shift over time (albeit performance is worse).\n",
    " \n",
    "\n",
    "In the multi-label case with full information, this is straight-forward - we just fit the algorithm on all the data and then make predictions, but this logic doesn't extend so well to censored labels (i.e. knowing only whether the label that was chosen as correct), as the better the policy, the more biased the data becomes, and new policies might just end up imitating the old one.\n",
    "\n",
    "The simplest approach would be to build a One-Vs-Rest classifier taking for each class only the data consisting of that action (if we know that observations always have at most 1 label, we can also take all the other data that got a reward and was not the action for which the classifier is being built as negative examples, but this is not a typical situation). However, we can also make use of the estimated rewards from the policy that generated the data in order to come up with less biased algorithms.\n",
    "\n",
    "The approaches implemented here are just for comparison purposes. In practice, the naive One-Vs-Rest approach can perform better than the approaches described here, especially for the case of discrete rewards, and typical settings such as online advertising call for online algorithms.\n",
    "\n",
    "** *\n",
    "<a id=\"p2\"></a>\n",
    "## 2. Algorithms\n",
    "\n",
    "The methods implemented here are:\n",
    "\n",
    "* `OffsetTree` (see _The offset tree for learning with partial labels_).\n",
    "\n",
    "* `DoublyRobustEstimator` (see _Doubly robust policy evaluation and learning_). Note that this method is meant for continuous rewards and doesn't work very well with discrete rewards. Also note that it is very computationally demanding.\n",
    " \n",
    "\n",
    "In the author's own words: \n",
    "\n",
    "Offset Tree:\n",
    "> The Offset Tree uses the following trick, which is easiest to understand in the case of k = 2 choices . When the observed reward $r_a$ of choice a is low, we essentially pretend that the other choice $a′$ was chosen and a different reward $r_{a′}$ was observed. Precisely how this is done and why, is driven by the regret analysis.\n",
    "\n",
    "Doubly-Robust Estimator:\n",
    "> Informally, the estimator uses (estimated_reward) as a baseline and if there is data available, a correction is applied. We will see that our estimator is accurate if at least one of the estimators (reward_estimate) and (probability_estimate), is accurate, hence the name doubly robust.\n",
    " \n",
    "\n",
    "Just like in the online module, these are also meta-heuristics that take a binary classification algorithm supporting sample weights as a base oracle. For the case of `DoublyRobustEstimator`, which converts the problem into cost-sensitive classification, you need to pass a regressor rather than a classifier when using method `RegressionOneVsRest` (picked by default).\n",
    "\n",
    "(For more information, see the references section at the end)\n",
    "\n",
    "** * \n",
    "<a id=\"p3\"></a>\n",
    "## 3. Experiments\n",
    "\n",
    "The experiments here are run on the same Bibtext dataset as in the guide for the online module.\n",
    "\n",
    "\n",
    "In order to simulate a stationary (and biased) data collection policy, I will fit a logistic regression model with a sample of the **fully-labeled** data, then let it choose actions for some more data, and take those actions and rewards as input for a new policy, along with the estimated reward probabilities for the actions that were chosen. Note that this is done just to choose the actions of the data on which further policies will be built, and the full labels will not be available to these policies.\n",
    "\n",
    "Some of the online algorithms in this package also allow an `output_score` argument to their `predict` function which can be used to generate this type of data (for more information see the individual documentation of each policy, e.g. `help(contextualbandits.online.BootstrappedTS)`).\n",
    "\n",
    "The evaluation is done with a test set with the full labels available. For the problem of evaluating policies based on partially-labeled data see the other IPython notebook [Policy Evaluation in Contextual Bandits](http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/policy_evaluation.ipynb).\n",
    "\n",
    "** * \n",
    "Loading the Bibtex dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Bibtex_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2d81ab6aabe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bibtex_data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2d81ab6aabe5>\u001b[0m in \u001b[0;36mparse_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minfoline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minfoline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^b'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfoline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Bibtex_data.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def parse_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        infoline = f.readline()\n",
    "        infoline = re.sub(r\"^b'\", \"\", str(infoline))\n",
    "        n_features = int(re.sub(r\"^\\d+\\s(\\d+)\\s\\d+.*$\", r\"\\1\", infoline))\n",
    "        features, labels = load_svmlight_file(f, n_features=n_features, multilabel=True)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(labels)\n",
    "    features = np.array(features.todense())\n",
    "    features = np.ascontiguousarray(features)\n",
    "    return features, labels\n",
    "\n",
    "features, y = parse_data(\"Bibtex_data.txt\")\n",
    "print(features.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating a stationary exploration policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c65743e3e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# separating the covariates data for each case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mXseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst_seed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mXexplore_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst_exploration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_exploration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# the 'explorer' polcy will be fit with this small sample of the rows\n",
    "st_seed = 0\n",
    "end_seed = 3000\n",
    "\n",
    "# then it will choose actions for this larger sample\n",
    "st_exploration = 0\n",
    "end_exploration = 5000\n",
    "\n",
    "# the new policy will be evaluated with a separate test set\n",
    "st_test = 5000\n",
    "end_test = 7395\n",
    "\n",
    "# separating the covariates data for each case\n",
    "Xseed = features[st_seed:end_seed, :]\n",
    "Xexplore_sample = features[st_exploration:end_exploration, :]\n",
    "Xtest = features[st_test:end_test, :]\n",
    "nchoices = y.shape[1]\n",
    "\n",
    "# now constructing an exploration policy as explained above, with fully-labeled data\n",
    "explorer = LogisticRegression(solver=\"lbfgs\", max_iter=15000)\n",
    "explorer.fit(Xseed, np.argmax(y[st_seed:end_seed], axis=1))\n",
    "\n",
    "# letting the exploration policy choose actions for the new policy input\n",
    "actions_explore_sample = explorer.predict(Xexplore_sample)\n",
    "rewards_explore_sample = y[st_exploration:end_exploration, :]\\\n",
    "                        [np.arange(end_exploration - st_exploration), actions_explore_sample]\n",
    "\n",
    "# extracting the probabilities it estimated\n",
    "ix_internal_actions = {j:i for i,j in enumerate(explorer.classes_)}\n",
    "ix_internal_actions = [ix_internal_actions[i] for i in actions_explore_sample]\n",
    "ix_internal_actions = np.array(ix_internal_actions)\n",
    "prob_actions_explore = explorer.predict_proba(Xexplore_sample)[np.arange(Xexplore_sample.shape[0]),\n",
    "                                                               ix_internal_actions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve solution: separate classifiers using subsets of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contextualbandits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d576faa8c72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeparateClassifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n\u001b[1;32m      5\u001b[0m                                  nchoices=y.shape[1], beta_prior=None, smoothing=None)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualbandits'"
     ]
    }
   ],
   "source": [
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                 nchoices=y.shape[1], beta_prior=None, smoothing=None)\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_naive = np.mean(y[st_test:end_test, :]\\\n",
    "                             [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers: \", mean_reward_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea from this same package: use a beta prior when the sample sizes are small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contextualbandits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a3af4868d558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeparateClassifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n\u001b[1;32m      5\u001b[0m                                  nchoices=y.shape[1], beta_prior=\"auto\")\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualbandits'"
     ]
    }
   ],
   "source": [
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                 nchoices=y.shape[1], beta_prior=\"auto\")\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_beta = np.mean(y[st_test:end_test, :]\\\n",
    "                            [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers + Prior: \", mean_reward_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contextualbandits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-614e26d303a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeparateClassifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n\u001b[1;32m      5\u001b[0m                                  nchoices=y.shape[1], beta_prior=None, smoothing = (1,2))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualbandits'"
     ]
    }
   ],
   "source": [
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                 nchoices=y.shape[1], beta_prior=None, smoothing = (1,2))\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_sm = np.mean(y[st_test:end_test, :]\\\n",
    "                            [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers + Smoothing: \", mean_reward_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying the offset tree method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contextualbandits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-17541ad2a42f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOffsetTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOffsetTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_algorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchoices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXexplore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_actions_explore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualbandits'"
     ]
    }
   ],
   "source": [
    "from contextualbandits.offpolicy import OffsetTree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = OffsetTree(base_algorithm=LogisticRegression(solver=\"lbfgs\", max_iter=15000), nchoices=y.shape[1])\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample, p=prob_actions_explore)\n",
    "mean_reward_ot = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Offset Tree technique: \", mean_reward_ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is quite similar to how it was before, and it didn't manage to bet the naive method. However, this is quite an unfair comparison, as there are many arms that the exploration policy didn't choose even once, so the offset tree has to sometimes decide between classes for which no data is available.\n",
    "** *\n",
    "The doubly-robust method can also be tried for the case of discrete rewards, where the reward estimates are the same probability estimates from the base algorithm. However, its performance is not as good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contextualbandits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-65b9cb73006b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextualbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoublyRobustEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mreward_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contextualbandits'"
     ]
    }
   ],
   "source": [
    "from contextualbandits.offpolicy import DoublyRobustEstimator\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "\n",
    "new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n",
    "                                   reward_estimator = LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                   nchoices = y.shape[1],\n",
    "                                   method = 'rovr', beta_prior = None, smoothing = None)\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample, p=prob_actions_explore)\n",
    "mean_reward_dr = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Doubly-Robust Estimator: \", mean_reward_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DoublyRobustEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ee419701a5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                    \u001b[0mreward_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                    \u001b[0mnchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                    method = 'rovr', beta_prior = \"auto\", smoothing = None)\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXexplore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_actions_explore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DoublyRobustEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n",
    "                                   reward_estimator = LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                   nchoices = y.shape[1],\n",
    "                                   method = 'rovr', beta_prior = \"auto\", smoothing = None)\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample, p=prob_actions_explore)\n",
    "mean_reward_dr_prior = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Doubly-Robust Estimator + Prior: \", mean_reward_dr_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DoublyRobustEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8c945fcd3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                    \u001b[0mreward_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                    \u001b[0mnchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                    method = 'rovr', beta_prior = None, smoothing = (1, 2))\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXexplore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards_explore_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_actions_explore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DoublyRobustEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "new_policy = DoublyRobustEstimator(base_algorithm = Ridge(),\n",
    "                                   reward_estimator = LogisticRegression(solver=\"lbfgs\", max_iter=15000),\n",
    "                                   nchoices = y.shape[1],\n",
    "                                   method = 'rovr', beta_prior = None, smoothing = (1, 2))\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample, p=prob_actions_explore)\n",
    "mean_reward_dr_sm = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Doubly-Robust Estimator + Smoothing: \", mean_reward_dr_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it also didn't manage to improve the estimates - which is not surprising given that the method is meant for the continuous reward scenario rather than the discrete rewards as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_reward_naive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4417ee06724b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     'Off-policy Learning Method' : ['Naive', 'Naive + Prior', 'Naive + Smoothing', 'Doubly-Robust',\n\u001b[1;32m      8\u001b[0m                                     'Doubly-Robust + Prior', 'Doubly-Robust + Smoothing', 'Offset Tree'],\n\u001b[0;32m----> 9\u001b[0;31m     'Test set mean reward' : [mean_reward_naive, mean_reward_beta, mean_reward_sm, mean_reward_dr,\n\u001b[0m\u001b[1;32m     10\u001b[0m                               mean_reward_dr_prior, mean_reward_dr_sm, mean_reward_ot]\n\u001b[1;32m     11\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_reward_naive' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt, pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Off-policy Learning Method' : ['Naive', 'Naive + Prior', 'Naive + Smoothing', 'Doubly-Robust',\n",
    "                                    'Doubly-Robust + Prior', 'Doubly-Robust + Smoothing', 'Offset Tree'],\n",
    "    'Test set mean reward' : [mean_reward_naive, mean_reward_beta, mean_reward_sm, mean_reward_dr,\n",
    "                              mean_reward_dr_prior, mean_reward_dr_sm, mean_reward_ot]\n",
    "})\n",
    "\n",
    "sns.set(font_scale = 1.3)\n",
    "rcParams['figure.figsize'] = 22, 7\n",
    "sns.barplot(x = \"Off-policy Learning Method\", y=\"Test set mean reward\", data=results)\n",
    "plt.title('Off-policy Learning on Bibtex Dataset\\nBase Classifier is Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** * \n",
    "<a id=\"p4\"></a>\n",
    "\n",
    "## References:\n",
    "\n",
    "* Beygelzimer, A., & Langford, J. (2009, June). The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 129-138). ACM.\n",
    "\n",
    "* Dudík, M., Langford, J., & Li, L. (2011). Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601.\n",
    "\n",
    "* Dudík, M., Erhan, D., Langford, J., & Li, L. (2014). Doubly robust policy evaluation and optimization. Statistical Science, 485-511."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
